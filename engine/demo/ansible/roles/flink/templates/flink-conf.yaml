################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  "License"); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################


#==============================================================================
# Common
#==============================================================================

# The host on which the JobManager runs. Only used in non-high-availability mode.
# The JobManager process will use this hostname to bind the listening servers to.
# The TaskManagers will try to connect to the JobManager on that host.

jobmanager.rpc.address: {{ inventory_hostname }}

taskmanager.hostname: {{ inventory_hostname }}

env.java.opts: "-Dflink.local.hostname={{ inventory_hostname }}"

# The port where the JobManager's main actor system listens for messages.

jobmanager.rpc.port: 6123


# The heap size for the JobManager JVM

jobmanager.heap.mb: {{jobmanager_heap_mb}}


# The heap size for the TaskManager JVM

taskmanager.heap.mb: {{taskmanager_heap_mb}}


# The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.

taskmanager.numberOfTaskSlots: {{taskmanager_slots}}


# The parallelism used for programs that did not specify and other parallelism.

parallelism.default: 8


#==============================================================================
# Web Frontend
#==============================================================================

# The port under which the web-based runtime monitor listens.
# A value of -1 deactivates the web server.

jobmanager.web.port: 8081


# The port uder which the standalone web client
# (for job upload and submit) listens.

webclient.port: 6080


#==============================================================================
# Streaming state checkpointing
#==============================================================================

# The backend that will be used to store operator state checkpoints if 
# checkpointing is enabled. 
#
# Supported backends: jobmanager, filesystem, <class-name-of-factory> 
#
#state.backend: filesystem


# Directory for storing checkpoints in a Flink-supported filesystem
# Note: State backend must be accessible from the JobManager and all TaskManagers.
# Use "hdfs://" for HDFS setups, "file://" for UNIX/POSIX-compliant file systems,
# (or any local file system under Windows), or "S3://" for S3 file system.
#
# state.backend.fs.checkpointdir: hdfs://namenode-host:port/flink-checkpoints


#==============================================================================
# Advanced
#==============================================================================

# The number of buffers for the network stack.
#
# taskmanager.network.numberOfBuffers: 2048


# Directories for temporary files.
#
# Add a delimited list for multiple directories, using the system directory
# delimiter (colon ':' on unix) or a comma, e.g.:
#     /data1/tmp:/data2/tmp:/data3/tmp
#
# Note: Each directory entry is read from and written to by a different I/O
# thread. You can include the same directory multiple times in order to create
# multiple I/O threads against that directory. This is for example relevant for
# high-throughput RAIDs.
#
# If not specified, the system-specific Java temporary directory (java.io.tmpdir
# property) is taken.
#
# taskmanager.tmp.dirs: /tmp


#domyslnie jest w tmp, a wtedy sa problemy z kasowaniem starych plikow
env.pid.dir: {{flink_dir}}

# Path to the Hadoop configuration directory.
#
# This configuration is used when writing into HDFS. Unless specified otherwise,
# HDFS file creation will use HDFS default settings with respect to block-size,
# replication factor, etc.
#
# You can also directly specify the paths to hdfs-default.xml and hdfs-site.xml
# via keys 'fs.hdfs.hdfsdefault' and 'fs.hdfs.hdfssite'.
#
# fs.hdfs.hadoopconf: /path/to/hadoop/conf/


#==============================================================================
# Master High Availability (required configuration)
#==============================================================================

# The list of ZooKepper quorum peers that coordinate the high-availability
# setup. This must be a list of the form:
# "host1:clientPort,host2[:clientPort],..." (default clientPort: 2181)
#
# recovery.mode: zookeeper
#
# recovery.zookeeper.quorum: localhost:2181,...
#
# Note: You need to set the state backend to 'filesystem' and the checkpoint
# directory (see above) before configuring the storageDir.
#
# recovery.zookeeper.storageDir: hdfs:///recovery

#nie potrzebujemy recovery na demo
#recovery.mode: zookeeper
#recovery.zookeeper.quorum: zk_host:2181
#recovery.zookeeper.path.root: /{{environment_id}}Flink # important: customize per cluster
#
#state.backend: filesystem
#state.backend.fs.checkpointdir: flink_storage_url/flink/checkpoints_{{environment_id}}
#recovery.zookeeper.storageDir: flink_storage_url/flink/recovery_{{environment_id}}

metrics.reporters: graphite_reporter
metrics.reporter.graphite_reporter.class: org.apache.flink.metrics.graphite.GraphiteReporter
metrics.reporter.graphite_reporter.host: {{graphite_host}}
metrics.reporter.graphite_reporter.port: {{graphite_port}}
metrics.reporter.graphite_reporter.protocol: UDP

metrics.scope.jm: {{environment_id}}.<host>.jobmanagerGlobal
metrics.scope.jm.job: {{environment_id}}.<host>.jobmanagerJob.<job_name>
metrics.scope.tm: {{environment_id}}.<host>.taskmanagerGlobal.<tm_id>
metrics.scope.tm.job: {{environment_id}}.<host>.taskmanagerJob.<tm_id>.<job_name>
metrics.scope.task: {{environment_id}}.<host>.taskmanagerTask.<tm_id>.<job_name>.<task_name>.<subtask_index>
metrics.scope.operator: {{environment_id}}.<host>.taskmanagerTask.<tm_id>.<job_name>.<operator_name>.<subtask_index>




